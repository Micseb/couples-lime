{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class to help select categorical vs. continuous data as part of the pipeline (see below)\n",
    "class DataSelector(BaseEstimator, TransformerMixin):\n",
    "    '''Select columns of numpy arrays based on attribute_indices.'''\n",
    "\n",
    "    def __init__(self, attribute_indices):\n",
    "        self.attribute_indices = attribute_indices\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array(X)[:,self.attribute_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load (preprocessed) data\n",
    "# \n",
    "# The raw data was downloaded from https://data.stanford.edu/hcmst and preprocessed.\n",
    "# We combined data sets collected across several years, we transformed select variables \n",
    "# (e.g., partner_education to be at the same level of granularity as education),\n",
    "# and added variables like the absolute age difference, education difference, etc.\n",
    "# Finally, we determined whether couples were still together (i.e., our labels).\n",
    "#\n",
    "# We provide the preprocessed data as a csv file in the same repo as this notebook.\n",
    "\n",
    "df = pd.read_csv('couples.csv')\n",
    "\n",
    "# Order features (numeric first, categorical second) since it's convenient later\n",
    "feature_order = ['age',\n",
    "                 'partner_age',\n",
    "                 'age_diff_abs',\n",
    "                 'children',\n",
    "                 'visits_relatives',\n",
    "                 'education',\n",
    "                 'marital_status',\n",
    "                 'partner_education',\n",
    "                 'gender',\n",
    "                 'house',\n",
    "                 'income',\n",
    "                 'msa',\n",
    "                 'rent',\n",
    "                 'political',\n",
    "                 'religion',\n",
    "                 'work',\n",
    "                 'gender_older',\n",
    "                 'education_difference',\n",
    "                 'success']\n",
    "\n",
    "data = df[feature_order]\n",
    "data = data[data.house != 'boat, rv, van, etc.'] # only one data point with this value, discard\n",
    "\n",
    "labels = data.pop('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peak at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define categorical names and indices\n",
    "categorical_features = list(data.columns[5:])\n",
    "categorical_idx = list(range(5, len(data.columns)))\n",
    "continuous_features = list(data.columns[0:5])\n",
    "continuous_idx = list(range(0,5))\n",
    "\n",
    "X = data.values\n",
    "\n",
    "# Get feature names and their values for categorical data (needed for LIME)\n",
    "categorical_names = {}\n",
    "for idx, feature in zip(categorical_idx, categorical_features):\n",
    "    le = LabelEncoder()\n",
    "    X[:, idx] = le.fit_transform(X[:, idx])\n",
    "    categorical_names[idx] = le.classes_\n",
    "\n",
    "# To suppress a warning later (not strictly necessary)\n",
    "X = X.astype(float)\n",
    "\n",
    "# Train test split\n",
    "train, test, labels_train, labels_test = train_test_split(\n",
    "    X, labels, train_size=0.70, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "#      \n",
    "# LIME needs a function that takes raw inputs and returns a prediction (see below).     \n",
    "# We use sklearn's pipeline to handle preprocessing, it simplifies the interaction with LIME (see below). \n",
    "# There are several ways to build this pipeline. For demo purposes, we here show the verbose option (and we\n",
    "# avoid scaling one-hot encoded features).\n",
    "\n",
    "continuous_pipeline = Pipeline([\n",
    "    ('selector', DataSelector(continuous_idx)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('selector', DataSelector(categorical_idx)),\n",
    "    ('encoder', OneHotEncoder(sparse=False)),\n",
    "    ])\n",
    "\n",
    "preprocessing_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"continuous_pipeline\", continuous_pipeline),\n",
    "    (\"categorical_pipeline\", categorical_pipeline),\n",
    "    ])\n",
    "\n",
    "# There are less verbose alternatives, especially if we scale one-hot encoded features,\n",
    "# an accepted practice in the machine learning community:\n",
    "#\n",
    "#     preprocessing_pipeline = Pipeline([\n",
    "#        ('onehotencoder', OneHotEncoder(categorical_features=categorical_idx, sparse=False)),\n",
    "#        ('scaler', StandardScaler())\n",
    "#     ])\n",
    "#\n",
    "# Finally, instead of the low-level Pipeline constructor, we can use sklearn's makepipeline:\n",
    "#\n",
    "#     preprocessing_pipeline = make_pipeline(\n",
    "#         OneHotEncoder(categorical_features=categorical_idx, sparse=False),\n",
    "#         StandardScaler()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model and GridSearch for random forest hyperparameter tuning\n",
    "param_grid = [{'n_estimators': [16, 20, 24],\n",
    "               'max_features': [4, 8, 12, 16],\n",
    "               'min_samples_split': [8, 12, 16],\n",
    "               'max_depth': [15, 20, 25]}]\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Extend the preprocessing pipeline to include random forest and grid search\n",
    "pipeline = make_pipeline(preprocessing_pipeline, grid_search)\n",
    "\n",
    "# Fit the model and tune the hyperparameters\n",
    "pipeline.fit(train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters found by GridSearchCV\n",
    "print(pipeline.named_steps['gridsearchcv'].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalute random forest classifier on training data (it overfits, small sample size)\n",
    "y_predict = pipeline.predict(train)\n",
    "f1 = f1_score(labels_train, y_predict)\n",
    "print('F1 on train:', f1)\n",
    "\n",
    "# Evalute random forest classifier on test data\n",
    "y_predict = pipeline.predict(test)\n",
    "f1 = f1_score(labels_test, y_predict)\n",
    "print('F1 on test:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances of random forest model (\"global interpretability\")\n",
    "best_estimator = pipeline.named_steps['gridsearchcv'].best_estimator_\n",
    "\n",
    "importances = best_estimator.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_estimator.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "feature_names = data.columns\n",
    "for f in range(data.shape[1]):\n",
    "    print(\"%2d. feature %s (%f)\" %\n",
    "          (f + 1, feature_names[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use LIME to explain individual predictions, initialize explainer object\n",
    "explainer = LimeTabularExplainer(\n",
    "    train,\n",
    "    class_names=['BrokeUp', 'StayedTogether'],\n",
    "    feature_names=list(data.columns),\n",
    "    categorical_features=categorical_idx,\n",
    "    categorical_names=categorical_names,\n",
    "    discretize_continuous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a prediction (\"local interpretability\"): \n",
    "# Now we see that the pipeline that takes raw data and returns the prediction \n",
    "# of the trained model now comes in conveniently.\n",
    "example = 3\n",
    "exp = explainer.explain_instance(test[example], pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain another prediction (\"local interpretability\"): \n",
    "example = 13\n",
    "exp = explainer.explain_instance(test[example], pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and we see differences in explaining the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LIME for for relationship management (not advised): Current chance of relationship success.\n",
    "current = [34, 36, 2, 0, 1, 0, 1, 0, 0, 0, 18, 0, 2, 0, 8, 4, 0, 0]\n",
    "exp = explainer.explain_instance(np.array(current), pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should I ask for a pay increase? It doesn't matter much.\n",
    "increase_income = [34, 36, 2, 0, 1, 0, 1, 0, 0, 0, 6, 0, 2, 0, 8, 4, 0, 0]\n",
    "exp = explainer.explain_instance(np.array(increase_income), pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should I buy a house? Maybe?\n",
    "buy_house = [34, 36, 2, 0, 1, 0, 1, 0, 0, 0, 6, 0, 1, 0, 8, 4, 0, 0]\n",
    "exp = explainer.explain_instance(np.array(buy_house), pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really, it's best to get married.\n",
    "get_married = [34, 36, 2, 0, 1, 0, 2, 0, 0, 0, 6, 0, 1, 0, 8, 4, 0, 0]\n",
    "exp = explainer.explain_instance(np.array(get_married), pipeline.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
